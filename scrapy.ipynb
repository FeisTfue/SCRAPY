{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "aOBWAurJGdAK",
        "outputId": "1efe6884-3b7b-4b68-cf87-6b259166cb38"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a3ebdefe08a7>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mcomment_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"No hay comentario disponible\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mnews_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mnews_bsobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_html\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    791\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m             )\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m         sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n\u001b[0m\u001b[1;32m    643\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m             \u001b[0mcert_reqs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcert_reqs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mserver_hostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m     ssl_sock = ssl_wrap_socket(\n\u001b[0m\u001b[1;32m    784\u001b[0m         \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m         \u001b[0mkeyfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mca_certs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mca_cert_dir\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mca_cert_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_verify_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mca_certs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca_cert_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca_cert_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "\n",
        "# Define la zona horaria de Perú\n",
        "peru_timezone = pytz.timezone('America/Lima')\n",
        "\n",
        "# Define la fecha de inicio (2014-01-01)\n",
        "start_date = datetime(2014, 1, 1)\n",
        "\n",
        "# Define el número de días que deseas recopilar\n",
        "num_days = 1\n",
        "\n",
        "# Calcula la fecha de finalización sumando el número de días a la fecha de inicio\n",
        "end_date = start_date + timedelta(days=num_days - 1)\n",
        "\n",
        "# Lista para almacenar los DataFrames de cada página\n",
        "dfs = []\n",
        "\n",
        "# Mientras la fecha de inicio sea anterior o igual a la fecha de finalización\n",
        "while start_date <= end_date:\n",
        "    # Formatea la fecha en el formato \"yyyy-mm-dd\"\n",
        "    formatted_date = start_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    # Construye la URL con la fecha actual\n",
        "    cnn_url = f\"https://elcomercio.pe/archivo/todas/{formatted_date}/\"\n",
        "\n",
        "    # Realiza la solicitud HTTP\n",
        "    html = requests.get(cnn_url)\n",
        "    bsobj = BeautifulSoup(html.content, 'lxml')\n",
        "\n",
        "    # Inicializa las listas para almacenar los datos\n",
        "    headlines = []\n",
        "    redaccion = []\n",
        "    dates = []\n",
        "    images = []\n",
        "    links = []\n",
        "    comments = []\n",
        "\n",
        "    # Tu código de extracción de datos aquí\n",
        "    for link in bsobj.findAll(\"h2\"):\n",
        "        headlines.append(link.text)\n",
        "\n",
        "    for news in bsobj.find_all('a', {'class': 'story-item__author block uppercase mt-10 font-thin text-xs text-gray-200'}):\n",
        "        author_name = news.text.strip()\n",
        "\n",
        "        # Reemplazar espacios en blanco con \"No hay\" si el nombre está vacío\n",
        "        if not author_name:\n",
        "            author_name = \"No hay\"\n",
        "\n",
        "        # Agregar el nombre a la lista\n",
        "        redaccion.append(author_name)\n",
        "\n",
        "    for news in bsobj.find_all('p', {'class': 'story-item__date font-thin ml-5 text-xs text-gray-300 md:mt-5 md:ml-0'}):\n",
        "        dates.append(news.text.strip())\n",
        "\n",
        "    for news in bsobj.find_all('img', {'class': 'story-item__img object-cover object-center w-full h-full'}):\n",
        "        img_url = news.get('src')\n",
        "        if img_url:\n",
        "            images.append(img_url)\n",
        "\n",
        "    for index, news in enumerate(bsobj.find_all('a', {'class': 'story-item__title block overflow-hidden primary-font line-h-xs mt-10'})):\n",
        "        news_link = news.get('href')\n",
        "        if news_link:\n",
        "            full_link = \"https://elcomercio.pe\" + news_link\n",
        "            links.append(full_link)\n",
        "            #print(full_link)  # Imprimir el enlace\n",
        "        else:\n",
        "            link_text = \"No tiene link\"\n",
        "            links.append(link_text)\n",
        "\n",
        "            # Imprimir el mensaje cuando no se encuentra un enlace\n",
        "            #print(f\"Iteración {index}: {link_text}\")\n",
        "\n",
        "    for link in links:\n",
        "        if link == \"No tiene link\":\n",
        "            comment_text = \"No hay comentario disponible\"\n",
        "        else:\n",
        "            news_html = requests.get(link)\n",
        "            news_bsobj = BeautifulSoup(news_html.content, 'lxml')\n",
        "\n",
        "            # Buscar el primer tipo de elemento que contiene el comentario\n",
        "            comment_element = news_bsobj.find('h2', {'class': 'sht__summary'})\n",
        "\n",
        "            # Si no se encontró el primer tipo de elemento, buscar el segundo tipo\n",
        "            if not comment_element:\n",
        "                comment_element = news_bsobj.find('h2', {'class': 'story-header__news-summary pr-20 pl-20 mb-20 secondary-font line-h-sm text-gray-300 text-xl font-normal'})\n",
        "\n",
        "            if comment_element:\n",
        "                comment_text = comment_element.text.strip()\n",
        "            else:\n",
        "                comment_text = \"No hay comentario disponible\"\n",
        "\n",
        "        comments.append(comment_text)\n",
        "        #print(comment_text)\n",
        "\n",
        "    print(len(headlines))\n",
        "    print(len(redaccion))\n",
        "    print(len(dates))\n",
        "    print(len(images))\n",
        "    print(len(links))\n",
        "    print(len(comments))\n",
        "\n",
        "    # Crear un DataFrame con los datos recopilados\n",
        "    df = pd.DataFrame({\n",
        "        'Headline': headlines,\n",
        "        'Redaccion': redaccion,\n",
        "        'Date': dates,\n",
        "        'Image URL': images,\n",
        "        'Link': links,\n",
        "        'Comentarios': comments\n",
        "    })\n",
        "\n",
        "    # Agregar el DataFrame a la lista de DataFrames\n",
        "    dfs.append(df)\n",
        "\n",
        "    # Incrementa la fecha de inicio para la siguiente iteración\n",
        "    start_date += timedelta(days=1)\n",
        "\n",
        "# Concatena todos los DataFrames en uno solo\n",
        "final_df = pd.concat(dfs, ignore_index=True)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "# Imprime el DataFrame final\n",
        "print(final_df)\n",
        "\n",
        "# Guarda el DataFrame final en un archivo CSV\n",
        "#final_df.to_csv('elcomercio_data4.csv', index=False)\n",
        "#final_df.to_excel('elcomercio_data4.xlsx', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "\n",
        "# Define la zona horaria de Perú\n",
        "peru_timezone = pytz.timezone('America/Lima')\n",
        "\n",
        "# Define la fecha de inicio (2014-01-01)\n",
        "start_date = datetime(2014, 1, 1)\n",
        "\n",
        "# Número de días para recopilar datos (0 para recopilar solo para la fecha inicial)\n",
        "num_days = 40\n",
        "\n",
        "# Si num_days es mayor que 0, calcula la fecha de finalización\n",
        "if num_days > 0:\n",
        "    end_date = start_date + timedelta(days=num_days - 1)\n",
        "else:\n",
        "    end_date = start_date\n",
        "\n",
        "# Lista para almacenar los DataFrames de cada página\n",
        "dfs = []\n",
        "\n",
        "# Mientras la fecha de inicio sea anterior o igual a la fecha de finalización\n",
        "while start_date <= end_date:\n",
        "    # Formatea la fecha en el formato \"yyyy-mm-dd\"\n",
        "    formatted_date = start_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    # Construye la URL con la fecha actual\n",
        "    cnn_url = f\"https://elcomercio.pe/archivo/todas/{formatted_date}/\"\n",
        "\n",
        "    # Realiza la solicitud HTTP\n",
        "    html = requests.get(cnn_url)\n",
        "    bsobj = BeautifulSoup(html.content, 'lxml')\n",
        "\n",
        "    # Inicializa las listas para almacenar los datos\n",
        "    headlines = []\n",
        "    redaccion = []\n",
        "    dates = []\n",
        "    images = []\n",
        "    links = []\n",
        "    comments = []\n",
        "\n",
        "    # Tu código de extracción de datos aquí\n",
        "    for link in bsobj.findAll(\"h2\"):\n",
        "        headlines.append(link.text)\n",
        "\n",
        "    for news in bsobj.find_all('a', {'class': 'story-item__author block uppercase mt-10 font-thin text-xs text-gray-200'}):\n",
        "        author_name = news.text.strip()\n",
        "\n",
        "        # Reemplazar espacios en blanco con \"No hay\" si el nombre está vacío\n",
        "        if not author_name:\n",
        "            author_name = \"No hay\"\n",
        "\n",
        "        # Agregar el nombre a la lista\n",
        "        redaccion.append(author_name)\n",
        "\n",
        "    for news in bsobj.find_all('p', {'class': 'story-item__date font-thin ml-5 text-xs text-gray-300 md:mt-5 md:ml-0'}):\n",
        "        dates.append(news.text.strip())\n",
        "\n",
        "    for news in bsobj.find_all('img', {'class': 'story-item__img object-cover object-center w-full h-full'}):\n",
        "        img_url = news.get('src')\n",
        "        if img_url:\n",
        "            images.append(img_url)\n",
        "\n",
        "    for index, news in enumerate(bsobj.find_all('a', {'class': 'story-item__title block overflow-hidden primary-font line-h-xs mt-10'})):\n",
        "        news_link = news.get('href')\n",
        "        if news_link:\n",
        "            full_link = \"https://elcomercio.pe\" + news_link\n",
        "            links.append(full_link)\n",
        "            #print(full_link)  # Imprimir el enlace\n",
        "        else:\n",
        "            link_text = \"No tiene link\"\n",
        "            links.append(link_text)\n",
        "\n",
        "            # Imprimir el mensaje cuando no se encuentra un enlace\n",
        "            #print(f\"Iteración {index}: {link_text}\")\n",
        "\n",
        "    for link in links:\n",
        "        if link == \"No tiene link\":\n",
        "            comment_text = \"No hay comentario disponible\"\n",
        "        else:\n",
        "            news_html = requests.get(link)\n",
        "            news_bsobj = BeautifulSoup(news_html.content, 'lxml')\n",
        "\n",
        "            # Buscar el primer tipo de elemento que contiene el comentario\n",
        "            comment_element = news_bsobj.find('h2', {'class': 'sht__summary'})\n",
        "\n",
        "            # Si no se encontró el primer tipo de elemento, buscar el segundo tipo\n",
        "            if not comment_element:\n",
        "                comment_element = news_bsobj.find('h2', {'class': 'story-header__news-summary pr-20 pl-20 mb-20 secondary-font line-h-sm text-gray-300 text-xl font-normal'})\n",
        "\n",
        "            if comment_element:\n",
        "                comment_text = comment_element.text.strip()\n",
        "            else:\n",
        "                comment_text = \"No hay comentario disponible\"\n",
        "\n",
        "        comments.append(comment_text)\n",
        "        #print(comment_text)\n",
        "\n",
        "    print(len(headlines))\n",
        "    print(len(redaccion))\n",
        "    print(len(dates))\n",
        "    print(len(images))\n",
        "    print(len(links))\n",
        "    print(len(comments))\n",
        "\n",
        "    # Crear un DataFrame con los datos recopilados\n",
        "    df = pd.DataFrame({\n",
        "        'Headline': headlines,\n",
        "        'Redaccion': redaccion,\n",
        "        'Date': dates,\n",
        "        'Image URL': images,\n",
        "        'Link': links,\n",
        "        'Comentarios': comments\n",
        "    })\n",
        "\n",
        "    # Agregar el DataFrame a la lista de DataFrames\n",
        "    dfs.append(df)\n",
        "\n",
        "    # Incrementa la fecha de inicio para la siguiente iteración\n",
        "    start_date += timedelta(days=1)\n",
        "\n",
        "# Concatena todos los DataFrames en uno solo\n",
        "final_df = pd.concat(dfs, ignore_index=True)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "# Imprime el DataFrame final\n",
        "print(final_df)\n",
        "\n",
        "# Guarda el DataFrame final en un archivo CSV\n",
        "final_df.to_csv('elcomercio_data3.csv', index=False)\n",
        "final_df.to_excel('elcomercio_data3.xlsx', index=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5895de-2HZFI",
        "outputId": "9e106261-ae25-4491-d7df-72fb36cfe773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n",
            "100\n"
          ]
        }
      ]
    }
  ]
}